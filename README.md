# AI Curriculum
Open Deep Learning and Reinforcement Learning lectures from top Universities like Stanford University, MIT, UC Berkeley.

<!-- toc -->

## Contents
- [Introduction to Deep Learning](#introduction-to-deep-learning)
  - MIT 6.S191: Introduction to Deep Learning | 2020
- [NLP with Deep Learning](#nlp-with-deep-learning)
  - CS224n: NLP with Deep Learning, Stanford | Winter 2019
- [Unsupervised Learning](#unsupervised-learning)
  - CS294-158-SP20: Deep Unsupervised Learning, UC Berkeley | Spring 2020
- [Machine learning | Stanford Autumn 2018 ]
  - https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU 
  - https://github.com/SKKSaikia/CS229_ML
- https://github.com/jeffheaton/t81_558_deep_learning

- https://github.com/ageron/handson-ml2
- https://github.com/curiousily/Getting-Things-Done-with-Pytorch
- https://github.com/chiphuyen/stanford-tensorflow-tutorials
- https://github.com/srivatsan88/Mastering-Apache-Spark

# Introduction to Deep Learning

### MIT 6.S191: Introduction to Deep Learning | 2020
[Lecture Series](http://introtodeeplearning.com/)

MIT's introductory course on deep learning methods with applications to computer vision, natural language processing, biology, and more! Students will gain foundational knowledge of deep learning algorithms and get practical experience in building neural networks in TensorFlow. Course concludes with a project proposal competition with feedback from staff and panel of industry sponsors. Prerequisites assume calculus (i.e. taking derivatives) and linear algebra (i.e. matrix multiplication), we'll try to explain everything else along the way! Experience in Python is helpful but not necessary. Listeners are welcome!

Source: MIT

[<p align="center"><img src="https://github.com/Machine-Learning-Tokyo/AI_Curriculum/blob/master/images/mit.gif" width="600"></p>](http://introtodeeplearning.com/)

# NLP with Deep Learning

### CS224n: NLP with Deep Learning, Stanford | Winter 2019
[Lecture Series](http://web.stanford.edu/class/cs224n/index.html#schedule)

Natural language processing (NLP) is a crucial part of artificial intelligence (AI), modeling how people share information. In recent years, deep learning approaches have obtained very high performance on many NLP tasks. In this course, students gain a thorough introduction to cutting-edge neural networks for NLP.

Source: Stanford

[<p align="center"><img src="https://github.com/Machine-Learning-Tokyo/AI_Curriculum/blob/master/images/nlp_stanford.png" width="600"></p>](http://web.stanford.edu/class/cs224n/index.html#schedule)

# Unsupervised Learning

### CS294-158-SP20: Deep Unsupervised Learning, UC Berkeley | Spring 2020
[Lecture Series, YouTube](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP)

This course covers two areas of deep learning in which labeled data is not required: Deep Generative Models and Self-supervised Learning. Recent advances in generative models have made it possible to realistically model high-dimensional raw data such as natural images, audio waveforms and text corpora. Strides in self-supervised learning have started to close the gap between supervised representation learning and unsupervised representation learning in terms of fine-tuning to unseen tasks. This course will cover the theoretical foundations of these topics as well as their newly enabled applications.

- Source: [UC Berkeley](https://sites.google.com/view/berkeley-cs294-158-sp20/home)

[<p align="center"><img src="https://github.com/Machine-Learning-Tokyo/AI_Curriculum/blob/master/images/deep_unsupervised.png" width="600"></p>](https://sites.google.com/view/berkeley-cs294-158-sp20/home)


# Multi-Task and Meta Learning

### Stanford CS330: Multi-Task and Meta Learning | 2019
[Lecture Series, YouTube](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5)

While deep learning has achieved remarkable success in supervised and reinforcement learning problems, such as image classification, speech recognition, and game playing, these models are, to a large degree, specialized for the single task they are trained for. This course will cover the setting where there are multiple tasks to be solved, and study how the structure arising from multiple tasks can be leveraged to learn more efficiently or effectively. This includes:

- goal-conditioned reinforcement learning techniques that leverage the structure of the provided goal space to learn many tasks significantly faster
- meta-learning methods that aim to learn efficient learning algorithms that can learn new tasks quickly
- curriculum and lifelong learning, where the problem requires learning a sequence of tasks, leveraging their shared structure to enable knowledge transfer

Source: [Stanford University CS330](http://cs330.stanford.edu/)

[<p align="center"><img src="https://github.com/Machine-Learning-Tokyo/AI_Curriculum/blob/master/images/meta_learning.png" width="600"></p>](http://cs330.stanford.edu/)
